From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: MC_XiaoHei <xiaohei.xor7studio@foxmail.com>
Date: Thu, 9 May 2024 22:49:36 +0800
Subject: [PATCH] Placeholder for Linear-region-file-format


diff --git a/.gitignore b/.gitignore
index 3811c0d849a3eb028ed1a6b7a2d4747f7f570448..1ad93453221244f880855b510e888e759275b640 100644
--- a/.gitignore
+++ b/.gitignore
@@ -46,3 +46,4 @@ dependency-reduced-pom.xml
 # vs code
 /.vscode
 /.factorypath
+
diff --git a/build.gradle.kts b/build.gradle.kts
index 3f1aa5dd24965127421deb7e8c00bb7d602274b9..5782f9c0361994b3699bf6624ddf182a75c32411 100644
--- a/build.gradle.kts
+++ b/build.gradle.kts
@@ -34,6 +34,10 @@ dependencies {
     alsoShade(log4jPlugins.output)
     implementation("io.netty:netty-codec-haproxy:4.1.97.Final") // Paper - Add support for proxy protocol
     // Paper end
+    // Leaves start - Linear format
+    implementation("com.github.luben:zstd-jni:1.5.5-11")
+    implementation("org.lz4:lz4-java:1.8.0")
+    // Leaves end - Linear format
     implementation("org.apache.logging.log4j:log4j-iostreams:2.22.1") // Paper - remove exclusion
     implementation("org.ow2.asm:asm-commons:9.7")
     implementation("org.spongepowered:configurate-yaml:4.2.0-SNAPSHOT") // Paper - config files
diff --git a/src/main/java/io/PaperFileIOThread.java b/src/main/java/io/PaperFileIOThread.java
new file mode 100644
index 0000000000000000000000000000000000000000..e69de29bb2d1d6434b8b29ae775ad8c2e48c5391
diff --git a/src/main/java/top/leavesmc/leaves/region/AbstractRegionFile.java b/src/main/java/top/leavesmc/leaves/region/AbstractRegionFile.java
new file mode 100644
index 0000000000000000000000000000000000000000..835b6d3726eda2573f616463d744cb5f6233e01c
--- /dev/null
+++ b/src/main/java/top/leavesmc/leaves/region/AbstractRegionFile.java
@@ -0,0 +1,45 @@
+package top.leavesmc.leaves.region;
+
+import net.minecraft.nbt.CompoundTag;
+import net.minecraft.world.level.ChunkPos;
+import net.minecraft.world.level.chunk.ChunkStatus;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.nio.file.Path;
+import java.util.concurrent.locks.ReentrantLock;
+
+public interface AbstractRegionFile {
+
+    void flush() throws IOException;
+
+    void clear(ChunkPos pos) throws IOException;
+
+    void close() throws IOException;
+
+    void setStatus(int x, int z, ChunkStatus status);
+
+    void setOversized(int x, int z, boolean b) throws IOException;
+
+    boolean hasChunk(ChunkPos pos);
+
+    boolean doesChunkExist(ChunkPos pos) throws Exception;
+
+    boolean isOversized(int x, int z);
+
+    boolean recalculateHeader() throws IOException;
+
+    DataOutputStream getChunkDataOutputStream(ChunkPos pos) throws IOException;
+
+    DataInputStream getChunkDataInputStream(ChunkPos pos) throws IOException;
+
+    CompoundTag getOversizedData(int x, int z) throws IOException;
+
+    ChunkStatus getStatusIfCached(int x, int z);
+
+    ReentrantLock getFileLock();
+
+    Path getRegionFile();
+}
+
diff --git a/src/main/java/top/leavesmc/leaves/region/AbstractRegionFileFactory.java b/src/main/java/top/leavesmc/leaves/region/AbstractRegionFileFactory.java
new file mode 100644
index 0000000000000000000000000000000000000000..980744f9b620a19fede7a6c7ce341bf5275a9b6a
--- /dev/null
+++ b/src/main/java/top/leavesmc/leaves/region/AbstractRegionFileFactory.java
@@ -0,0 +1,30 @@
+package top.leavesmc.leaves.region;
+
+import net.minecraft.world.level.chunk.storage.RegionFile;
+import net.minecraft.world.level.chunk.storage.RegionFileVersion;
+
+import java.io.IOException;
+import java.nio.file.Path;
+
+public class AbstractRegionFileFactory {
+
+    public static AbstractRegionFile getAbstractRegionFile(int linearCompression, Path file, Path directory, boolean dsync) throws IOException {
+        return getAbstractRegionFile(linearCompression, file, directory, RegionFileVersion.VERSION_DEFLATE, dsync);
+    }
+
+    public static AbstractRegionFile getAbstractRegionFile(int linearCompression, Path file, Path directory, boolean dsync, boolean canRecalcHeader) throws IOException {
+        return getAbstractRegionFile(linearCompression, file, directory, RegionFileVersion.VERSION_DEFLATE, dsync, canRecalcHeader);
+    }
+
+    public static AbstractRegionFile getAbstractRegionFile(int linearCompression, Path file, Path directory, RegionFileVersion outputChunkStreamVersion, boolean dsync) throws IOException {
+        return getAbstractRegionFile(linearCompression, file, directory, outputChunkStreamVersion, dsync, false);
+    }
+
+    public static AbstractRegionFile getAbstractRegionFile(int linearCompression, Path file, Path directory, RegionFileVersion outputChunkStreamVersion, boolean dsync, boolean canRecalcHeader) throws IOException {
+        if (file.toString().endsWith(".linear")) {
+            return new LinearRegionFile(file, linearCompression);
+        } else {
+            return new RegionFile(file, directory, outputChunkStreamVersion, dsync, canRecalcHeader);
+        }
+    }
+}
diff --git a/src/main/java/top/leavesmc/leaves/region/LinearRegionFile.java b/src/main/java/top/leavesmc/leaves/region/LinearRegionFile.java
new file mode 100644
index 0000000000000000000000000000000000000000..072495e6c0c08a3239faab0fb6ebb28451039694
--- /dev/null
+++ b/src/main/java/top/leavesmc/leaves/region/LinearRegionFile.java
@@ -0,0 +1,328 @@
+package top.leavesmc.leaves.region;
+
+import com.github.luben.zstd.ZstdInputStream;
+import com.github.luben.zstd.ZstdOutputStream;
+import com.mojang.logging.LogUtils;
+import net.jpountz.lz4.LZ4Compressor;
+import net.jpountz.lz4.LZ4Factory;
+import net.jpountz.lz4.LZ4FastDecompressor;
+import net.minecraft.nbt.CompoundTag;
+import net.minecraft.world.level.ChunkPos;
+import net.minecraft.world.level.chunk.ChunkStatus;
+import org.slf4j.Logger;
+
+import javax.annotation.Nullable;
+import java.io.*;
+import java.nio.ByteBuffer;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.StandardCopyOption;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.locks.ReentrantLock;
+
+// Powered by LinearPurpur(https://github.com/StupidCraft/LinearPurpur)
+public class LinearRegionFile implements AbstractRegionFile, AutoCloseable {
+
+    private static final long SUPERBLOCK = -4323716122432332390L;
+    private static final byte VERSION = 2;
+    private static final int HEADER_SIZE = 32;
+    private static final int FOOTER_SIZE = 8;
+    private static final Logger LOGGER = LogUtils.getLogger();
+    private static final List<Byte> SUPPORTED_VERSIONS = Arrays.asList((byte) 1, (byte) 2);
+    private static final LinearRegionFileFlusher linearRegionFileFlusher = new LinearRegionFileFlusher();
+
+    private final byte[][] buffer = new byte[1024][];
+    private final int[] bufferUncompressedSize = new int[1024];
+
+    private final int[] chunkTimestamps = new int[1024];
+    private final ChunkStatus[] statuses = new ChunkStatus[1024];
+
+    private final LZ4Compressor compressor;
+    private final LZ4FastDecompressor decompressor;
+
+    public final ReentrantLock fileLock = new ReentrantLock(true);
+    private final int compressionLevel;
+
+    private final AtomicBoolean markedToSave = new AtomicBoolean(false);
+    public boolean closed = false;
+    public Path path;
+
+
+    public LinearRegionFile(Path file, int compression) throws IOException {
+        this.path = file;
+        this.compressionLevel = compression;
+        this.compressor = LZ4Factory.fastestInstance().fastCompressor();
+        this.decompressor = LZ4Factory.fastestInstance().fastDecompressor();
+
+        File regionFile = new File(this.path.toString());
+
+        Arrays.fill(this.bufferUncompressedSize, 0);
+
+        if (!regionFile.canRead()) return;
+
+        try (FileInputStream fileStream = new FileInputStream(regionFile);
+             DataInputStream rawDataStream = new DataInputStream(fileStream)) {
+
+            long superBlock = rawDataStream.readLong();
+            if (superBlock != SUPERBLOCK) {
+                throw new RuntimeException("Invalid superblock: " + superBlock + " in " + file);
+            }
+
+            byte version = rawDataStream.readByte();
+            if (!SUPPORTED_VERSIONS.contains(version)) {
+                throw new RuntimeException("Invalid version: " + version + " in " + file);
+            }
+
+            // Skip newestTimestamp (Long) + Compression level (Byte) + Chunk count (Short): Unused.
+            rawDataStream.skipBytes(11);
+
+            int dataCount = rawDataStream.readInt();
+            long fileLength = file.toFile().length();
+            if (fileLength != HEADER_SIZE + dataCount + FOOTER_SIZE) {
+                throw new IOException("Invalid file length: " + this.path + " " + fileLength + " " + (HEADER_SIZE + dataCount + FOOTER_SIZE));
+            }
+
+            rawDataStream.skipBytes(8); // Skip data hash (Long): Unused.
+
+            byte[] rawCompressed = new byte[dataCount];
+            rawDataStream.readFully(rawCompressed, 0, dataCount);
+
+            superBlock = rawDataStream.readLong();
+            if (superBlock != SUPERBLOCK) {
+                throw new IOException("Footer superblock invalid " + this.path);
+            }
+
+            try (DataInputStream dataStream = new DataInputStream(new ZstdInputStream(new ByteArrayInputStream(rawCompressed)))) {
+                int[] starts = new int[1024];
+                for (int i = 0; i < 1024; i++) {
+                    starts[i] = dataStream.readInt();
+                    dataStream.skipBytes(4); // Skip timestamps (Int): Unused.
+                }
+
+                for (int i = 0; i < 1024; i++) {
+                    if (starts[i] > 0) {
+                        int size = starts[i];
+                        byte[] b = new byte[size];
+                        dataStream.readFully(b, 0, size);
+
+                        int maxCompressedLength = this.compressor.maxCompressedLength(size);
+                        byte[] compressed = new byte[maxCompressedLength];
+                        int compressedLength = this.compressor.compress(b, 0, size, compressed, 0, maxCompressedLength);
+                        b = new byte[compressedLength];
+                        System.arraycopy(compressed, 0, b, 0, compressedLength);
+
+                        this.buffer[i] = b;
+                        this.bufferUncompressedSize[i] = size;
+                    }
+                }
+            }
+        }
+    }
+
+    public Path getRegionFile() {
+        return this.path;
+    }
+
+    public ReentrantLock getFileLock() {
+        return this.fileLock;
+    }
+
+    public void flush() throws IOException {
+        if (isMarkedToSave()) flushWrapper(); // sync
+    }
+
+    private void markToSave() {
+        linearRegionFileFlusher.scheduleSave(this);
+        markedToSave.set(true);
+    }
+
+    public boolean isMarkedToSave() {
+        return markedToSave.getAndSet(false);
+    }
+
+    public void flushWrapper() {
+        try {
+            save();
+        } catch (IOException e) {
+            LOGGER.error("Failed to flush region file " + path.toAbsolutePath(), e);
+        }
+    }
+
+    public boolean doesChunkExist(ChunkPos pos) throws Exception {
+        throw new Exception("doesChunkExist is a stub");
+    }
+
+    private synchronized void save() throws IOException {
+        long timestamp = getTimestamp();
+        short chunkCount = 0;
+
+        File tempFile = new File(path.toString() + ".tmp");
+
+        try (FileOutputStream fileStream = new FileOutputStream(tempFile);
+             ByteArrayOutputStream zstdByteArray = new ByteArrayOutputStream();
+             ZstdOutputStream zstdStream = new ZstdOutputStream(zstdByteArray, this.compressionLevel);
+             DataOutputStream zstdDataStream = new DataOutputStream(zstdStream);
+             DataOutputStream dataStream = new DataOutputStream(fileStream)) {
+
+            dataStream.writeLong(SUPERBLOCK);
+            dataStream.writeByte(VERSION);
+            dataStream.writeLong(timestamp);
+            dataStream.writeByte(this.compressionLevel);
+
+            ArrayList<byte[]> byteBuffers = new ArrayList<>();
+            for (int i = 0; i < 1024; i++) {
+                if (this.bufferUncompressedSize[i] != 0) {
+                    chunkCount += 1;
+                    byte[] content = new byte[bufferUncompressedSize[i]];
+                    this.decompressor.decompress(buffer[i], 0, content, 0, bufferUncompressedSize[i]);
+
+                    byteBuffers.add(content);
+                } else {
+                    byteBuffers.add(null);
+                }
+            }
+            for (int i = 0; i < 1024; i++) {
+                zstdDataStream.writeInt(this.bufferUncompressedSize[i]); // Write uncompressed size
+                zstdDataStream.writeInt(this.chunkTimestamps[i]); // Write timestamp
+            }
+            for (int i = 0; i < 1024; i++) {
+                if (byteBuffers.get(i) != null) {
+                    zstdDataStream.write(byteBuffers.get(i), 0, byteBuffers.get(i).length);
+                }
+            }
+            zstdDataStream.close();
+
+            dataStream.writeShort(chunkCount);
+
+            byte[] compressed = zstdByteArray.toByteArray();
+
+            dataStream.writeInt(compressed.length);
+            dataStream.writeLong(0);
+
+            dataStream.write(compressed, 0, compressed.length);
+            dataStream.writeLong(SUPERBLOCK);
+
+            dataStream.flush();
+            fileStream.getFD().sync();
+            fileStream.getChannel().force(true); // Ensure atomicity on Btrfs
+        }
+        Files.move(tempFile.toPath(), this.path, StandardCopyOption.REPLACE_EXISTING);
+    }
+
+
+    public void setStatus(int x, int z, ChunkStatus status) {
+        this.statuses[getChunkIndex(x, z)] = status;
+    }
+
+    public synchronized void write(ChunkPos pos, ByteBuffer buffer) {
+        try {
+            byte[] b = toByteArray(new ByteArrayInputStream(buffer.array()));
+            int uncompressedSize = b.length;
+
+            int maxCompressedLength = this.compressor.maxCompressedLength(b.length);
+            byte[] compressed = new byte[maxCompressedLength];
+            int compressedLength = this.compressor.compress(b, 0, b.length, compressed, 0, maxCompressedLength);
+            b = new byte[compressedLength];
+            System.arraycopy(compressed, 0, b, 0, compressedLength);
+
+            int index = getChunkIndex(pos.x, pos.z);
+            this.buffer[index] = b;
+            this.chunkTimestamps[index] = getTimestamp();
+            this.bufferUncompressedSize[getChunkIndex(pos.x, pos.z)] = uncompressedSize;
+        } catch (IOException e) {
+            LOGGER.error("Chunk write IOException " + e + " " + this.path);
+        }
+        markToSave();
+    }
+
+    public DataOutputStream getChunkDataOutputStream(ChunkPos pos) {
+        return new DataOutputStream(new BufferedOutputStream(new ChunkBuffer(pos)));
+    }
+
+    private class ChunkBuffer extends ByteArrayOutputStream {
+
+        private final ChunkPos pos;
+
+        public ChunkBuffer(ChunkPos chunkcoordintpair) {
+            super();
+            this.pos = chunkcoordintpair;
+        }
+
+        public void close() {
+            ByteBuffer bytebuffer = ByteBuffer.wrap(this.buf, 0, this.count);
+            LinearRegionFile.this.write(this.pos, bytebuffer);
+        }
+    }
+
+    private byte[] toByteArray(InputStream in) throws IOException {
+        ByteArrayOutputStream out = new ByteArrayOutputStream();
+        byte[] tempBuffer = new byte[4096];
+
+        int length;
+        while ((length = in.read(tempBuffer)) >= 0) {
+            out.write(tempBuffer, 0, length);
+        }
+
+        return out.toByteArray();
+    }
+
+    @Nullable
+    public synchronized DataInputStream getChunkDataInputStream(ChunkPos pos) {
+        if (this.bufferUncompressedSize[getChunkIndex(pos.x, pos.z)] != 0) {
+            byte[] content = new byte[bufferUncompressedSize[getChunkIndex(pos.x, pos.z)]];
+            this.decompressor.decompress(this.buffer[getChunkIndex(pos.x, pos.z)], 0, content, 0, bufferUncompressedSize[getChunkIndex(pos.x, pos.z)]);
+            return new DataInputStream(new ByteArrayInputStream(content));
+        }
+        return null;
+    }
+
+    public ChunkStatus getStatusIfCached(int x, int z) {
+        return this.statuses[getChunkIndex(x, z)];
+    }
+
+    public void clear(ChunkPos pos) {
+        int i = getChunkIndex(pos.x, pos.z);
+        this.buffer[i] = null;
+        this.bufferUncompressedSize[i] = 0;
+        this.chunkTimestamps[i] = getTimestamp();
+        markToSave();
+    }
+
+    public boolean hasChunk(ChunkPos pos) {
+        return this.bufferUncompressedSize[getChunkIndex(pos.x, pos.z)] > 0;
+    }
+
+    public void close() throws IOException {
+        if (closed) {
+            return;
+        }
+        closed = true;
+        flush(); // sync
+    }
+
+    private static int getChunkIndex(int x, int z) {
+        return (x & 31) + ((z & 31) << 5);
+    }
+
+    private static int getTimestamp() {
+        return (int) (System.currentTimeMillis() / 1000L);
+    }
+
+    public boolean recalculateHeader() {
+        return false;
+    }
+
+    public void setOversized(int x, int z, boolean something) {
+    }
+
+    public CompoundTag getOversizedData(int x, int z) throws IOException {
+        throw new IOException("getOversizedData is a stub " + this.path);
+    }
+
+    public boolean isOversized(int x, int z) {
+        return false;
+    }
+}
diff --git a/src/main/java/top/leavesmc/leaves/region/LinearRegionFileFlusher.java b/src/main/java/top/leavesmc/leaves/region/LinearRegionFileFlusher.java
new file mode 100644
index 0000000000000000000000000000000000000000..fd8ec703b1be35ef3c29afd4abe2dfaf8bdc5c61
--- /dev/null
+++ b/src/main/java/top/leavesmc/leaves/region/LinearRegionFileFlusher.java
@@ -0,0 +1,52 @@
+package top.leavesmc.leaves.region;
+
+import com.google.common.util.concurrent.ThreadFactoryBuilder;
+
+import java.util.Queue;
+import java.util.concurrent.*;
+
+import org.bukkit.Bukkit;
+import top.leavesmc.leaves.LeavesConfig;
+
+// Powered by LinearPurpur(https://github.com/StupidCraft/LinearPurpur)
+public class LinearRegionFileFlusher {
+    private final Queue<LinearRegionFile> savingQueue = new LinkedBlockingQueue<>();
+    private final ScheduledExecutorService scheduler = Executors.newSingleThreadScheduledExecutor(
+        new ThreadFactoryBuilder()
+            .setNameFormat("linear-flush-scheduler")
+            .build()
+    );
+    private final ExecutorService executor = Executors.newFixedThreadPool(
+        LeavesConfig.getLinearFlushThreads(),
+        new ThreadFactoryBuilder()
+            .setNameFormat("linear-flusher-%d")
+            .build()
+    );
+
+    public LinearRegionFileFlusher() {
+        Bukkit.getLogger().info("Using " + LeavesConfig.getLinearFlushThreads() + " threads for linear region flushing.");
+        scheduler.scheduleAtFixedRate(this::pollAndFlush, 0L, LeavesConfig.linearFlushFrequency, TimeUnit.SECONDS);
+    }
+
+    public void scheduleSave(LinearRegionFile regionFile) {
+        if (savingQueue.contains(regionFile)) {
+            return;
+        }
+        savingQueue.add(regionFile);
+    }
+
+    private void pollAndFlush() {
+        while (!savingQueue.isEmpty()) {
+            LinearRegionFile regionFile = savingQueue.poll();
+            if (!regionFile.closed && regionFile.isMarkedToSave()) {
+                executor.execute(regionFile::flushWrapper);
+            }
+        }
+    }
+
+    public void shutdown() {
+        executor.shutdown();
+        scheduler.shutdown();
+    }
+}
+
diff --git a/src/main/java/top/leavesmc/leaves/region/RegionFileFormat.java b/src/main/java/top/leavesmc/leaves/region/RegionFileFormat.java
new file mode 100644
index 0000000000000000000000000000000000000000..a9a8e79557fac57e29b64e7e55fc04b80ad11ae3
--- /dev/null
+++ b/src/main/java/top/leavesmc/leaves/region/RegionFileFormat.java
@@ -0,0 +1,14 @@
+package top.leavesmc.leaves.region;
+
+public enum RegionFileFormat {
+    ANVIL, LINEAR, INVALID;
+
+    public static RegionFileFormat fromString(String format) {
+        for (RegionFileFormat regionFileFormat : values()) {
+            if (regionFileFormat.name().equalsIgnoreCase(format)) {
+                return regionFileFormat;
+            }
+        }
+        return RegionFileFormat.INVALID;
+    }
+}
